
<html>
<head>
<title> CS585 Project  </title>
<style>
<!--
body{
font-family: 'Trebuchet MS', Verdana;
}
p{
font-family: 'Trebuchet MS', Times;
margin: 10px 10px 15px 20px;
}
h3{
margin: 5px;
}
h2{
margin: 10px;
}
h1{
margin: 10px 0px 0px 20px;
}
div.main-body{
align:center;
margin: 30px;
}
hr{
margin:20px 0px 20px 0px;
}

table.center {
    margin-left: auto;
    margin-right: auto;
}
-->
</style>
</head>

<body>
<center>
<a href="http://www.bu.edu"><img border="0" src="http://www.cs.bu.edu/fac/betke/images/bu-logo.gif"
width="119" height="120"></a>
</center>

<h1>Emotion Detection</h1>

<p> 
 Davide Lucchi <br>
 Mona Jalal<br>
 Silvia Ionescu<br>
 12/7/2017
</p>

<br>
<h2><a href="https://drive.google.com/open?id=14xfbT8Kl0Tatkaj_6DtMJrs0naPZdOrltExR7xDzlw4">Link to Presentation</a></h2>
<hr>

<p class="main-body">

<h3> Project Problem Definition </h3>
<p>
Given the video of a person classify the emotion that their face is showing in each frame. 
The algorithm will be applied to the videos of the 2016 presidential debate between Hillary Clinton and Donald
Trump where each video shows only one candidate. Our target emotions are
neutral, happy, sad, surprise, fear, disgust and anger.

</p>
<p>
    Emotion recognition has many applications like smart home automation, self-driving cars,
    improving the classroom dynamics based on the students' emotions, and helping people with disabilities based on their perceived affections. Emotion/expression
    recognition is studied under the umbrella of affective computing as a way for the human-computer interaction (<b>HCI</b>) as well as human-robot interaction (<b>HRI</b>).
    Emotion recognition is one of the harder tasks in computer vision tasks spectrum with respect to achieved accuracy.
    The problem of emotion/expression recognition would be even harder when studied in other modalities like face sketches, face caricature and/or
    when special filters are applied that has deformed/malformed the face in a way detectable by a human and not probably a computer.
    Most of the efforts in the emotion recognition literature has been performed in normal human faces and only for a very few (7) emotions.
    Additionally, most of the research in the community has been geared towards winning the challenges for emotion recognition like
    EmotiW (Emotion Recognition in the Wild Challenge), DCER and HPE (Joint Challenge on Dominant and Complementary Emotion
    Recognition Using Micro-Emotion Features and Head-Pose Estimation), FERA (Facial Expression Recognition and Analysis Challenge), MEC (
    Multimodal Emotion Recognition Challenge) [17], EmotiW 5.0 [18], and AVEC (Audio/Visual Emotion Challenge) [1]. All these challenges come up with their own dataset. Additional
    famous datasets for emotion recognition are FER2013, CK+, MMI facial expression database, KDEF [9] and AffectNet. In this project, we worked with FER2013, KDEF and
    AffectNet datasets.
    Overall, what makes a good dataset depends on various factors like having exposure to colored images of various age groups, ethnicity groups, and genders besides
    having cleaned and organized labels. Indeed one of the datasets that we came across (EmotionNet) which included the Facial Action Units encoding for each image,
    was very hard to work with and needed substantial data cleaning time compared to the other three datasets we worked with. We would additionally like to bring
    the attention of those who make datasets available to researcher to how easy-to-access datasets need to be. We had quite our share of numerous challenges for
    retrieving the 120GB AffectNet dataset. Additionally, FER2013, which we assume is the most prevalent dataset in the field, has very low-resolution images many of
    which include watermark in the images. These watermark potentially can degrade the prediction accuracy of machine learning and deep learning systems.
</p>
<p>
    EmotiW [15] challenge goal is to provide a standard platform for emotion recognition.
    The first EmotiW (2013), brought the attention of researchers to emotion recognition in <b>Acted Facial Expression in the Wild</b> (AFEW). For example, the EmotiW 2013 winners used a deep neural network (DNN) method for detecting emotions. They used a convolutional neural network (CNN) for face analysis,
    bag of words method for analysing the mouth area, and use of deep belief networks for audio signal processing.In EmotiW 2015,
    a new emotion recognition challenge was introduced with the emerge of <b>Static Facial Expression in the Wild</b> (SFEW) database. SFEW database is extracted
    from AFEW database using a fiducial point-based clustering method.
    The EmotiW 2015 highest classification accuracy of 54% for AFEW and 62% for SFEW
    represents the needs for further research that needs to be done by the researchers in the field to improve the accuracy of emotion recognition. Reseachers have tried to
    improve the emotion recognition from various corners including enhancing the accuracy of face and face part detection which plays the most crucial role in emotion
    recognition, improving the head pose detection, as well as sorting out the problems with varied illumination, noise and lack of labeled data. The last problem has been
    focus of many recent works in which they have used crowdsourcing where they ask crowd-workers to label the face emotions in large amount as done in AffectNet.
    Some of the future meta-data that could be included in the datasets studied by researcher that could improve the accuracy of emotion recognition are as EEG, eye gaze estimation and eye gaze direction,
    and skin galvanic response. Moreover, recent attention has been geared towards group-level emotion recognition as in EmotiW 5.0 challenge in which they try to analyze
    the effect of a group of people in images.


</p>
<hr>




<h3>Input data</h3>
<p>Hillary Clinton and Donald Trump's Presidential Debate on October 19th, 2016
    <br>
     This dataset is given to us by Professor Margrit Betke's research group. 
     The video is modified so that the video is cropped in half so that we can see the individual candidates. 
</p>



<table class="center">

<tr>

<td align="center" valign="middle">Donald Trump</td>
<td align="center" valign="middle">Hilary Clinton</td>
</tr>
<tr>
<td>
<video width="320" height="240" controls>
  <source src="videos/Donald_Trump.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</td>
<td>
<video width="320" height="240" controls>
 <source src="videos/Hilary_Clinton.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</td>
</tr>
</table>
</p>

<hr>


<h3>Desired Results</h3>

<p>
The output of the algorithm should be a set of scores of the emotion in the candidate's face. 
This scores represent the probability that the current espression is associated to that
emotion. The scores are recomputed for each set of frames.

<table class="image" align="center">
<tr>
	<td><img width="200px" src="img/H1.jpg"></td>
	<td><img width="200px" src="img/H2.jpg"></td>
	<td><img width="200px" src="img/H3.jpg"></td>
</tr>
<tr>
	<td><img width="200px" src="img/T1.jpg"></td>
	<td><img width="200px" src="img/T2.jpg"></td>
	<td><img width="200px" src="img/T3.jpg"></td>
</tr>

</table>


</p>

<hr>
<h3>Related Work</h3>

<p>
Facial expressions are one of the most important non-verbal ways that human beings convey internal emotion.
This means that there have been significant efforts to develop reliable automated face expression recognition (FER) systems
that can understand human emotion and can interact with humans more naturally. One of the main problems for these systems is the
fact that they have to operate in uncontrolled environments where the scene lighting, camera view, image resolution, background, 
users pose can have significant variations. With the rise of deep learning systems and big data systems, we are now able to have training
    data in the order of 500k image samples which are precisely annotated using multiple Amazon Mechanical Turk crowd workers for each image. Having large
    training dataset helps us with being able to shoot for very deep neural networks that can learn new patterns and detect emotions while the
    conventional computer vision algorithms or shallow neural networks might fail.
</p>
<p>
There have been a few models developed to quantify facial expressions and behaviors:
<ul>
  <li> Categorical model - emotion is chosen from a list of categories [4]</li>
  <li> Dimensional model - a value is chosen over a continuous emotional scale (valence and arousal)[5] </li>
  <li> Facial Action Coding System (FACS) model - all possible facial actions are described in terms of Action Units (AUs)[6]</li>
 
</ul>
</p>

<p>
Datasets of facial expressions in the wild have received a spacial attention due to the uncontrolled environment 
setting that FER systems have be applied to.   
</p>

<p>
The Facial Expression Recognition 2013 (FER-2013) datatset was introduced in the ICML 2013 challenge [11].
The database was created using the Google image search API that matched a set of 184 emotion-related keywords to 
capture the six basic expressions as well as the neutral expression. Images were resized to 48x48 pixels and
converted to grayscale. Human labelers were used to reject incorrectly labeled images and filter out some duplicate images.
The resulting database contains 35,887 images most of which are in the wild settings (collected from all over the Web).
    The downside of the FER-2013 dataset
is that the faces are not registered, and only a small number of images portray disgust (547 images) hence the emotion distribution is not close to uniform, facial landmark
detectors fail to extract facial landmarks at this resolution and quality. Winner of the FER challenge obtained a 71.2% accuracy
on the test set by using CNNs with linear one-vs-all SVM classifier at the top.
</p>



<p>
AffectNet[3] is a large database containing more than 1M facial images collected from the Internet by querying
three major search engines using 1250 emotion related keywords in six different languages. Half of the retrieved images
(~440K) were manually annotated for the presence of seven discrete facial expressions (categorial model) 
and the intensity of valence and arousal (dimensional model). 
</p>


<hr>

<h2>Methods</h2>

<p>
Support Vector Machines (SVMs) and Convolutional Neural Networks(CNNs) were applied to the three datasets listed below. Action units were extracted from 
the KDEF and FER-2013 datasets and Support Vector Machines(SVM) was applied for classification. Convolutional Neural Networks (CNNs) were applied to Fer-2013 
AffectNet.
</p>

<p>
<table border="1" width=50%  style="center " align="center">
<tbody>
<tr>
<td>DataSet</td>
<td>Source</td>
<td># of Images</td>
<td>Condition</td>
<td>Categories</td>
</tr>
<td>KDEF</td>
<td></td>
<td>4900</td>
<td> Controlled/Posed</td>
<td>7 emotions</td>
</tr>
<tr>
<td>FER-2013</td>
<td>ICML 2013 challenge [11]</td>
<td>35,887</td>
<td> Wild </td>
<td>7 emotions</td>
</tr>
<tr>
<td>AffectNet</td>
<td>Mollahosseini/Hasani/Mahoor[3]</td>
<td>450,000</td>
<td> Wild </td>
<td>7 emotions</td>
</tr>

</tbody>
</table>
</p>


<p>
Support Vector Machines (SVM) are deterministic supervised learning models used for classification. SVM classifies data by finding the best hyperplane that separates
two classes while maximizing the margin between the two classes. The SVM optimization problem is shown below, where w is the weight vector, b is the bias and i = 1; ::;N
</p>


<p>
<table class="results", align="center">
<tr>
	<td><img src="img/SVM.jpg"></img></td>
</tr>

</table>
</p>

<p>
Convolutional Neural Networks(CNNs) architecture  implemented was a VGG16 as shown below. 
The convolutional layers apply 3x3 filters to the images with the number of filters varying per layer, maxPooling layer has a pool size of 2, and the fully connected layer 
has a size of 512. Dropout layers of 0.4, batch normalization, and l2 regularization were applied to each convolutional layer. 
</p>

<p>
<figure>
<center>
<img src="img/VGG_architecture.jpg"></img>
<img src="img/VGG_architecture_2.jpg"></img>
<figcaption>Fig. 1 - Convolutional Neural Network Architecture (VGG16) </figcaption>
</figure>
</center>
</p>


<hr>
 <h2>Experimental Setup</h2>

<h3>1.<u>SVM</u></h3>
<p>
    For this part we started with a baseline SVM code [13] that detects emotion based on 68 facial landmarks.
We implemented a SVM with facial action units. The Action Units were extracted with
 <a href="https://github.com/TadasBaltrusaitis/OpenFace">OpenFace</a> [8]
from the datasets FER2013 and Karolinska Directed Emotional Faces (KDEF).
The KDEF dataset consists of 4900 pictures of 70 individuals each displaying 7 different emotional expressions. Each expressions is
photographed from 5 different angles. The fact that the debate images have high resolution and that
the candidates are for most of the time facing the camera under good lighting conditions suggest 
that a wild dataset is probably not needed to obtain a good emotion prediction accuracy and a simpler
dataset like KDEF could be helpful enough.
The AU output of OpenFace consists of a set of occurences and intensities one for each of the AU that OpenFace is
able to recognize. You can refer to the Appendix section for an elaborate visualization of AUs and their
    relation to emotions. For some of the images given the position of person's face, it
 was not possible to calculate the AUs. This happened usually on side pictures
where OpenFace was not able to recognize the face. This reduced even more the size of the dataset which in the end was 3022 images.
The same problem occurred with the FER2013 dataset for which it was possible to generate only 5200 images.
The reason about this is that the low-resolution of small size (48x48 pixels) images made it more difficult for OpenFace to
generate the AUs and when this was not possible, no output file was created.
 For each dataset image the AUs were computed and the intensities and occurences 
values were put in a vector. The SVM training data consisted of a matrix with such vectors. The SVM used is a Support
Vector Classification (SVC) from Python sklearn library with a rbf (radial basis function) kernel and ovr (One Vs Rest) classifier. The training and validation data were generated
with the function train_test_split from sklearn in order to have a random sample.
</p>


Here are some of the landmark results created by OpenFace Docker container FaceLandmarkImg script on AffectNet dataset:

<h4><p>Images with Correct Landmark</p></h4>
<table align="center">
    <tbody>
    <tr>
        <td><img src="img/04c782cd3d0955de01d0f98c8c3401889c5d5541f271ce582d7c176d_det_0.jpg" width="200px" height="200px"></td>
        <td><img src="img/06ac5a2a81ce0790d305bfc19ffc1bbc70bef692667a713d3df1cdb3_det_0.jpg" width="200px" height="200px"></td>
        <td><img src="img/0676100ea8c9dbd1c91a270c3214629a5a852634f6151b244849c969_det_0.jpg" width="200px" height="200px"></td>
    </tr>
    </tbody>
</table>


<h4><p>Images with Wrong Landmark</p></h4>
<table align="center">
    <tbody>
    <tr>
        <td><img src="img/1147fa4f094646a67a31a71f3ab19dcb9d1602cc9bc76855424d27b0_det_0.jpg" width="200px" height="200px"></td>
        <td><img src="img/131ae1ecab06d140df980236bad93824a409ae1ff3d676fc166ca67d_det_0.jpg" width="200px" height="200px"></td>
        <td><img src="img/087c4974b9c75b1f34f5cf772b6b1896c06f23917bce3a46148f908c_det_0.jpg" width="200px" height="200px"></td>
    </tr>
    </tbody>
</table>



<h4><p>Model trained on FER2013</p></h4>
<p>
This model was trained on the AUs from the FER2013 website. The total data consisted of 5200 images.
This data was divided into a train and test dataset where the test dataset comprised 15% of the images.
The SVM was run with <b>10000 epochs</b> and it gave a <b>validation accuracy of 45.3%</b> and a <b>f1 score of 0.4218</b>. 
This results represent an improvement over the baseline SVM we started with. Such SVM in fact was using 
facial landmarks to predict emotions and could achieve an accuracy of only <b>39.2%</b> while being trained on the 
entire FER2013 dataset. 
The output model was then used to predict on the images from the debate. The video below is a collection of frames where the emotion has
been predicted by the SVM. 

</p>





<h4><p>Model trained on KDEF</p></h4>
<p>
The same method was used with this dataset. In this case the dataset consisted of 3022 images where 
the validation dataset comprised 15% of the total images.
The code was run with <b>10000 epochs</b> and it gave a <b>validation accuracy of 67.4%</b> and a <b>f1 score of 0.6709</b>.
 The output model was then used to predict on the images from the debate. The video below is a collection of frames where the emotion has
been predicted by the SVM. 

</p>



<br>


<h3>2.<u>Convolutional Neural Networks(CNNs)</u></h3>
<h4><p><u>A. FER-2013 Dataset</p></u></h4>

<p>
The FER-2013 dataset consists of 35,887 images that are queried from the web. The data consists of 48x48 pixel grayscale images of faces. 
The faces have been automatically registered so that the face is centered and occupies about the same amount of space in each image. 
The task is to categorize each face based on the emotion shown in the facial expression in to one of seven categories 
(0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral). The training set consists of 28,709 examples and test set has a size of 3,589 examples. 
</p>

<p>
For the FER-2013 dataset, the VGG16 architecture(shown above) was implemented using Keras with tensorflow backend. 
This architecture was tested with both data augmentation and without using Keras ImageDataGenerator. Our test accuracy results increased from 60% to 65% with the data augmentation, therefore 
we continued to use data augmentation for our next optimization section. A rotation_range=15, width_shift_range=0.1, height_shift_range=0.1 was used for the image augmentation parameters. 
</p>
<p>
The starting point in the CNN experimental setup was to test a range of leaning rates for the stochastic gradient descent optimizer. Fig. 2 shows the test accuracy 
and loss results for four different learning rates (0.1, 0.05, 0.01, 0.005) over 200 epochs. 
We noticed that the test accuracy increases as the learning rate decreases to 0.01 and 0.005, however we notice that the test loss for these two leaning rates starts increasing slightly 
after 150 epochs. 
     
</p>

<p>
<figure>
<center>
<img src="img/Fer_2013_test_acc_lr_comp.jpg" width="350px" height="350px"></img>
<img src="img/Fer_2013_test_loss_lr_comp.jpg" width="350px" height="350px"></img>
<figcaption>Fig. 2 - FER2013 test accuracy and loss plots vs. epochs for leaning rates = 0.1, 0.05. 0.01, 0.005 </figcaption>
</figure>
</center>
</p>



<p>
Because of the increase in the test loss for learning rates 0.01 and 0.005 after 150 epochs, we decided to try adding a 0.5 dropout layer and l2 regularization after each convolution
layer. Fig. 3 below shows that the test accuracy remains the same ~ 68%, however the test loss has a smaller increase as the number of epochs increases. 
Due to this test results we decided to use a 0.5 dropout layer and l2 regularization in our architecture moving forward.
</p>

<p>
<figure>
<center>
<img src="img/FER_2013_Test_Acc_dropout.jpg" width="350px" height="350px"></img>
<img src="img/Fer_2013_Test_Loss_dropout.jpg" width="350px" height="350px"></img>
<figcaption>Fig. 3 - FER2013 test accuracy and loss plots vs. epochs. Dropout layer/l2 regularization and without comparison.</figcaption>
</figure>
</center>
</p>



<p> 
Due to the increase in test accuracy as the learning rate decreases, we decided to start with a learning rate of 0.1 and decrease it by half every 25 epochs. 
This was done in combination with a dropout layer, batch normalization, and l2 regularization at each convolution layer. This hyper-parameters 
setup gave us a test accuracy of 69.3%. Fig. 4 displays these results for the train and test accuracy and loss as a function of epochs. We notice from the 
figures below that our CNN are overfitting, however this issue could not be overcome by data augmentation, dropout layer or regularization.  

</p>

<p>
<figure>
<center>
<img src="img/Fer_2013_Train_Test_Accuracy_lr_gradual.jpg" width="350px" height="350px""></img>
<img src="img/Fer_2013_Train_Test_Loss_lr_gradual.jpg" width="350px" height="350px"></img>
<figcaption>Fig. 4 - FER2013 train and test accuracy and loss plots vs. # of epochs.</figcaption>
</figure>
</center>
</p>


<h4><p><u>B. AffectNet Dataset</p></u></h4>


<p>
AffectNet is a large dataset containing ~450,000(60GB) RGB images that were manually annotated, and taken in wild conditions, which means that are pulled from the web. 
This dataset has 11 categories for emotion classification (0=Neutral, 1=Happy, 2=Sad, 3=Surprise, 4=Fear, 5=Disgust, 6=Anger, 7=Contempt, 8=None, 9=Uncertain, 10=Non-face).
</p>

<p> 
For the training set, we initially pulled ~210,000 images from only the first 8 classes of emotion (0=Neutral, 1=Happy, 2=Sad, 3=Surprise, 4=Fear, 5=Disgust, 6=Anger, 7=Contempt). 
These images were resized to 48x48 size, cropped to contain only the face, 
and transformed to grayscale. The test set had a size of 4,000 images with classifications belonging to the first 8 classes of emotion. 
Using the VGG16 network described earlier in combination with dropout layers, batch normalization, and l2 normalization we tried testing for different learning rates 0.1, 0.05, 0.001.
The best test accuracy of 50% after 100 epochs is obtained using a learning rate of 0.05 for these hyperparameters, as shown in Fig. 5.
 </p>

<p>
<figure>
<center>
<img src="img/AffectNet_Test_Accuracy_Grayscale.jpg" width="350px" height="350px""></img>
<img src="img/AffectNet_Test_Loss_Grayscale.jpg" width="350px" height="350px"></img>
<figcaption>Fig. 5 - AffectNet test accuracy and loss plots vs. # of epochs for grayscale images.</figcaption>
</figure>
</center>
</p>

<p>
For AffectNet, we also tried using RGB images for training the CNN network. In order to speed up the training process, we pulled only 37k and 64k images for the training set from AffectNet
representing only 8 classes and tested 2 different learning rates. The results for the RGB images are shown below and this setup gave us a test accuracy of 55%. 

</p>
<p>
<figure>
<center>
<img src="img/AffectNet_Test_Accuracy_RGB.jpg" width="350px" height="350px"></img>
<img src="img/AffectNet_Test_Loss.jpg" width="350px" height="350px""></img>
<figcaption>Fig. 6 - AffectNet test accuracy and loss plots vs. # of epochs for RGB images.</figcaption>
</figure>
</center>
</p>

<hr>
<h2>Results and Discussion</h2>


<h3><u>1.SVM Comparison</u></h3>
<p>
One every four frames was extracted from each input video and for each of them the SVM models
were used to predict the emotion in that frame. 
The following tables illustrate the predictions obtained over a sample of 1372 frames from the debate. 

</p>

<table align="center">
<tbody>
<tr>
<td><img src="img/trump_ist.jpg" width="600px"></td>
<td><img src="img/clinton_ist.jpg" width="600px"></td>
</tr>
</tbody>
</table>


<p> 
The predictions highlight the differences and similarities between the two models. In particular,
for Trump the biggest disagreement was between sad and angry. The FER model in fact interpreted most
of the non neutral expressions as sad while the KDEF as angry. A few of such frames are shown below.

</p>
<h4><p>FER2013</p></h4>
<table align="center">
<tbody>
<tr>
<td><img src="img/image22_fer.jpg" width="200px" height="200px"></td>
<td><img src="img/image58_fer.jpg" width="200px" height="200px"></td>
<td><img src="img/image1222_fer.jpg" width="200px" height="200px"></td>
</tr>
</tbody>
</table>
    
	
<h4><p>KDEF</p></h4>
<table align="center">
<tbody>
<tr>
<td><img src="img/image22_kdfe.jpg" width="200px" height="200px"></td>
<td><img src="img/image58_kdfe.jpg" width="200px" height="200px"></td>
<td><img src="img/image1222_kdfe.jpg" width="200px" height="200px"></td>
</tr>
</tbody>
</table>

<p>
In this specific examples Trump's face seem to be more angry than sad and the results are consistent also in the 
other frames. The KDEF model therefore seems to be more accurate.

The two models on the other hand agree on the neutral expression and 
seem to recognize them very accurately. Some examples are provided below.

</p>
<h4><p>Frames both models predicted as neutral</p></h4>
<table align="center">
<tbody>
<tr>
<td><img src="img/image261.jpg" width="200px" height="200px"></td>
<td><img src="img/image318.jpg" width="200px" height="200px"></td>
<td><img src="img/image417.jpg" width="200px" height="200px"></td>
</tr>
</tbody>
</table>
<p>
Another important difference is in the surprised emotion. A sample of such pictures is given for
both models.
</p>
<h4><p>FER2013</p></h4>
<table align="center">
<tbody>
<tr>
<td><img src="img/image252_fer.jpg" width="200px" height="200px"></td>
<td><img src="img/image894_fer.jpg" width="200px" height="200px"></td>
<td><img src="img/image1091_fer.jpg" width="200px" height="200px"></td>
</tr>
</tbody>
</table>
    
	
<h4><p>KDEF</p></h4>
<table align="center">
<tbody>
<tr>
<td><img src="img/image945_kdfe.jpg" width="200px" height="200px"></td>
<td><img src="img/image949_kdfe.jpg" width="200px" height="200px"></td>
<td><img src="img/image1163_kdfe.jpg" width="200px" height="200px"></td>
</tr>
</tbody>
</table>
<p>
Looking at the pictures it is possible to see that FER2013 is more accurate. 
Even though Trump's expression in that debate moment may be just a consequence of his way of talking 
the pictures do portray a surprised expression which the FER2013 model is able to recognize. KDEF instead
performs poorly and predicts surprised even for pictures that are quite different from each other.
This suggests that the model did not have enough training data for this emotion. A larger dataset would 
probably solve this problem. 
For Trump's emotions the results suggest that none of the two models is perfect. Better results
could be achieved by merging the two datasets. 
For the surprised emotion it would be better to use only the 
images provided by FER2013 while for angry only the images of KDEF.
</p>

<p> 
The same analysis can be carried out for Hillary Clinton as well. Here
the biggest difference is betwen the sad and surprise emotions. Interestingly 
the values are about simmetric that is it seems that what FER predicted as sad KDFE predicted as
surprise. This turned out to be not the case though. In fact, many of the pictures labeled
as Sad by FER where labeled as Neutral by KDFE. This result explains the difference in 
the percentages of images classified as Neutral. A few examples are provided below.
</p>

<h4><p>FER2013</p></h4>
<table align="center">
<tbody>
<tr>
<td><img src="img/image432_fer.jpg" width="200px" height="200px"></td>
<td><img src="img/image544_fer.jpg" width="200px" height="200px"></td>
<td><img src="img/image931_fer.jpg" width="200px" height="200px"></td>
</tr>
</tbody>
</table>
    
	
<h4><p>KDEF</p></h4>
<table align="center">
<tbody>
<tr>
<td><img src="img/image432_kdfe.jpg" width="200px" height="200px"></td>
<td><img src="img/image544_kdfe.jpg" width="200px" height="200px"></td>
<td><img src="img/image931_kdfe.jpg" width="200px" height="200px"></td>
</tr>
</tbody>
</table>
<p>
From these samples KDFE seems to be more accurate. Those faces in fact do not really portray sadness. 
The model trained on FER2013 therefore proved to be ineffective at classifing sadness as both 
Trump's and Hilary's images are misclassified. This could be due to the small number of sad faces in the 
training dataset which prevented the model to learn this emotion effectively.  


The two models on the other hand classify correctly neutral expressions. Some examples are provided below.
</p>

<h4><p>Frames both models predicted as neutral</p></h4>

<table align="center">
<tbody>
<tr>
<td><img src="img/image942_net.jpg" width="200px" height="200px"></td>
<td><img src="img/image380_net.jpg" width="200px" height="200px"></td>
<td><img src="img/image432_net.jpg" width="200px" height="200px"></td>
</tr>
</tbody>
</table>
<p>
The KDEF model classified almost 30% of the images as surprised. A sample is given below.

</p>
<table align="center">
<tbody>
<tr>
<td><img src="img/image1061_sup.jpg" width="200px" height="200px"></td>
<td><img src="img/image990_sup.jpg" width="200px" height="200px"></td>
<td><img src="img/image940_sup.jpg" width="200px" height="200px"></td>
</tr>
</tbody>
</table>
   
  <p>
The first image does portray Surprise but the other two do not. This suggests that the model is actually 
able to classify correclty the images that show surprise but the false positives rate is very high. This information 
is consistent with the Trump images as well. 


</p>

<h4><p>Gifs with results for FER2013 model</p></h4>
<p>
<table align="center">
<tbody>
<tr>
<td><img src="https://media.giphy.com/media/3o6fJdxRkIchKfDazC/giphy.gif" width="400px"></td>
<td><img src="https://media.giphy.com/media/3ohs7NQ1cToLOe4ka4/giphy.gif" width="415px"></td>
</tr>
</tbody>
</table>
</p>

<h4><p>Gifs with results for KDEF model</p></h4>
<p>
<table align="center">
<tbody>
<tr>
<td><img src="https://media.giphy.com/media/xT0xeELBaXDuQQqFe8/giphy.gif" width="400px"></td>
<td><img src="https://media.giphy.com/media/xUOxfccxGvxSXTDbZS/giphy.gif" width="415px"></td>
</tr>
</tbody>
</table>
</p>
<p>
Even if the expressions of Hilary Clinton and Donald Trump are quite different the two models
behave in a consistent way. The most accurate emotions are Happyness and Neutral for both datasets. KDFE 
classifies Angry very well. As for Surprise the SVM trained on FER2013 behaves better in general but tends to have many 
false positives. It was not possible to test the correctness of Disgust and Fear as both candidates
do not show such emotions. Both models labeled only very few images with Afraid and none with Disgust 
which suggests that the models do not assign those labels randomly but the data is not enough to make any conclusion.


</p>


<h3><u>2. Convolutional Neural Networks(CNNs)</u></h3>

<p>
As presented in the experimental setup section our CNN was better optimized for the FER2013 dataset(best test accuracy of 69%) compared to AffectNet(best test accuracy of 55%).
Due to this reason we decided to use the FER2013 trained weights to classify the input videos of Hillary Clinton and Donald Trump. The hyper-parameters used to train the convolutional 
neural network weights on the FER2013 dataset are shown in the below.    
</p>

<table align="center" border="1">
<tbody>
<tr>
	<td>Hyper-parameter</td>
	<td>Value</td>
<tr>
	<td>Learning Rate</td>
	<td>Start at 0.1 and decrease every 25 epochs</td>
</tr>
<tr>
	<td>Dropout Layer</td>
	<td>0.5</td>

</tr>
<tr>
	<td>L2 Regularization</td>
	<td>0.0005</td>
</tr>

<tr>
	<td>Batch size</td>
	<td>128</td>
</tr>
<tr>
	<td>Momentum</td>
	<td>0.9</td>
</tr>

<tr>
	<td>Rotation Range</td>
	<td>0.15</td>
</tr>

<tr>
	<td>Width Shift Range</td>
	<td>0.1</td>
</tr>

<tr>
	<td>Hight Shift Range</td>
	<td>0.1</td>
</tr>


</tbody>
</table>


<p>
As previously specified the FER2013 training set consists of 28,709 examples and test set has a size of 3,589 examples. The test accuracy for this dataset with thsese hyper-parameters 
was 69.3%. The test accuracy results are shown in Fig. 4 and the confusion matrix results for each individual emotion are shown below in Fig. 7. 
</p>

</p>
<p>
<figure>
<center>
<img src="img/FER-2013_Test_confusion_matrix_250epochs.jpg" width="600px" height="450px"></img>
<figcaption>Fig. 7 - FER2013 confusion matrix.</figcaption>
</figure>
</center>
</p>

<p>
As we can see from the confusion matrix, the sad and fear emotion classification have the lowest accuracy. 
We noticed that the sad classification is often misclassified as fear or neutral and that the fear classification is most often misclassified as sad and neutral.  
</p>

<p>
Using the trained weights from the FER2013 dataset we classified the input images of Hillary and Trump. A sample of our classification results for Hillary Clinton are shown below. 
</p>

<table align="center">
<tbody>
<tr>
<td><img src="img/CNN_Hillary_results.jpg" width="550px" height="500px"></td>
</tr>

</tbody>
</table>

<p>
The image classification for Hillary Clinton for the neutral, happy, surprised, and disgust classification work well, however the fear and sad classification seam not properly classified. 
</p>

<p>
 A sample of our classification results for Donald Trump have also been performed and are shown below.
</p>

<table align="center">
<tbody>
<tr>
<td><img src="img/CNN_Trump_results.jpg" width="550px" height="500px"></td>
</tr>

</tbody>
</table>

<p>
For the Donald Trump image classification, the angry and sad classified images look very similar.  Also, the image classified as surprised looks more angry to us. 
</p>


<h4><p>FER2013 CNN results</p></h4>

<table class="center">

<tr>

<td align="center" valign="middle">Hilary Clinton</td>
<td align="center" valign="middle">Donald Trump</td>
</tr>
<tr>
<td>
<video width="320" height="240" controls>
  <source src="videos/Hilary_classified.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</td>
<td>
<video width="320" height="240" controls>
 <source src="videos/Trump_classified.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</td>
</tr>
</table>



Videos of 100 classified frames of both Hillary Clinton and Donald Trump using our convolutional neural network trained weights are shown above.
The input videos were not previously classified, therefore we can not provide an accuracy for these classification. 
These frames would have to be annotated by multiple people in order to properly provide an accuracy.  


<hr>
<h2>Conclusion</h2>

<p>

Emotion recognition could have many applications like smart home automation, self-driving cars, improving the classroom dynamics, and helping people with 
disabilities based on their perceived affections. However, emotion detection and classification is a difficult problem to tackle since humans express emotion in 
complex and diverse ways. We have attempted to classify emotion through two methods: Action Units/SVM and Convolutional Neural Networks. 
</p>

<p>
Action units were extracted from two datasets FER2013 and KDEF and classified using Support Vector Machines(SVM). 
For the FER2013 dataset we obtained  validation accuracy of 45.3% and a f1 score of 0.4218.
The KDEF dataset performed better with a validation accuracy of 67.4% and a f1 score of 0.6709  after 10000 epochs. 
The KDEF dataset performed better because the images were taken in a controlled/posed setting as opposed to the FER2013, which is a wild dataset.   
</p> 

<p>
For the Convolutional Neural Network method, two datasets were tested FER2013 and AffectNet. We decided not to try it on KDEF datased due to the small number of images in this 
datasest(4,900), which would have had to be split into training and testing sets.  In terms of the datasets that we used, the FER2013 had test accuracy of 69.3% using grayscale images. 
For AffectNet, we obtained a test accuracy of 55% using RGB images. The reason for this difference in results while applying the same convolutional neural network architecture and similar 
hyperparameters is because we used 8 emotion classes for AffectNet as opposed to 7 emotion classes for FER2013. This could also mean that the CNN hyper-parameters were not optimized 
for the AffectNet dataset and that more parameter tuning needed to be performed. 

AffectNet is a much bigger dataset ~450, 000 images and even thou 
these images were manually annotated it could be that some of them were misclassified. Also, since  AffectNet images were collected from the Internet by querying three major search 
engines using 1250 emotion related keywords in six different languages, it is possible that the classes of emotions are providing a wider range of images for that emotion the FER2013. 
This could mean that we need to implement a different type of CNN architecture, such as Resnet50, in order to increase our accuracy.    
</p>

<p>
As a final conclusion, for the FER-2013 dataset the CNNs performed better then SVMs and action units.
The hyper-parameter tunings for the CNNs affect our results significantly and the 
results from one dataset to another one are not transferable by default.
</p>



<hr>
<h3>Future Works</h3>
In what follows, we suggest some future works some of which we had in our initial plan but we didn't get the chance to do
those experiments due to the time constraints of the project. Some of the other suggestions are based on the experiments we
did and came across the idea of using or took them away from the papers we studied and during our brain-storming sessions, decided
they would be a great fit for our project.
<ul>
    <li>
        Use of Recurrent Neural Networks (RNNs) for detecting emotions in videos.
    </li>
    <li>
        Multi-modal emotion recognition using both audio and video signals.
    </li>
    <li>
        Use of the state-of-the-art DNN architectures as pre-trained models like ResNet50 or DenseNet121 instead of VGG16
        to accelerate the task of training.
    </li>
    <li>
        Experimenting with different types of ReLU (like Leaky ReLU) as well as different optimizers (Adam vs. SGD vs. RMSProp)
    </li>
    <li>
        Drawing saliency maps [19] to realize which pixels participate most for various emotions which would help us in debugging
        misclassification errors.
    </li>
    <li>
        Using AFEW and SFEW datasets which seem more of an standard in the emotion recognition community (We learned about
        these towards the end of project).
    </li>
    <li>
        Checking to see if our algorithms would be able to detect emotions in group-level as for EmotiW 5.0 challenge.
    </li>
    <li>
        Using YOLO v2 for the face detection phase as it is more accurate compared to OpenFace. Also, YOLO v2 could be used
        for real-time face detection which can help in creating a real-time emotion detector.
    </li>
    <li>
        Experimenting with different kernel functions in the SVM like Gaussian or polynomial kernels besides RBF.
    </li>
</ul>

<hr>

<h3>References</h3>
<ol>
    <!-- silvia please have a look at first reference! -->
    <li>
        <b>AVEC 2016, Depression, Mood, and Emotion</b>, The 6th Audio/Visual Emotion Challenge and Workshop,
        <a href="http://sspnet.eu/avec2016/">http://sspnet.eu/avec2016/</a>, 2016.
    </li>


<li>A. Mollahosseini, B. Hasani, M. J. Salvador, H. Abdollahi, D. Chan, and M. H. Mahoor, <b>Facial Expression Recognition from
World Wild Web</b>,in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2016.
</li>

<li>A. Mollahosseini, B. Hasani, and M. H. Mahoor, <b>AffectNet: A New Database for Facial Expression, Valence,
    and Arousal Computation in the Wild</b>, IEEE Transactions on Affective Computing, 2017.
</li>
    <li>P. Ekman, and W. V. Friesen, <b>Constants across Cultures in the Face and Emotion.</b> Journal of personality and
social psychology,vol. 17, no. 2, p. 124, 1971.
    </li>
    <li>J. A. Russell, <b>A Circumplex Model of Affect</b>, Journal of Personality and Social Psychology,
 vol. 39, no. 6, pp. 1161-1178, 1980.
    </li>
    <li>P. Ekman and W. V. Friesen, <b>Facial Action Coding System</b>, 1977.</li>

    <li>
        J. Deng, N. Cummins, J. Han, X. Xu, Z. Ren, V. Pandit, Z. Zhang, and B. Schuller, <b>The University of Passau Open Emotion Recognition System for the Multimodal Emotion Challenge</b>, in Chinese Conference on Pattern Recognition. Singapore, SGP: Springer, 2016, pp. 652-666.
    </li>
    <li>
        T. Baltrusaitis, P. Robinson, and L. P. Morency, <b><a href="https://github.com/TadasBaltrusaitis/OpenFace">OpenFace</a>: An Open Source Facial Behavior Analysis Toolkit. in IEEE Winter Conference on Applications of Computer Vision</b>, 2016.
    </li>
	<li>
        D. Lundqvist, A. Flykt, and A. Ohman, <b>The Karolinska Directed Emotional Faces - <a href="http://www.emotionlab.se/resources/kdef">KDEF</a></b>, CD-ROM from Department of Clinical Neuroscience, Psychology section, Karolinska Institutet, ISBN 91-630-7164-9, 1998.
    </li>

    <li>
        Y. Tang, <b>Deep Learning using Linear Support Vector Machines</b>, CoRR abs/1306.0239 (2013).
    </li>

    <li>
        <b>Kaggle FER2013 Challenge</b>, <a href="https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data">https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data</a>, 2013.
    </li>
    <li>
        <b>Dlib Shape Predictor Model</b>, <a href="http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2">http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2</a>
    </li>
    <li>
        <b>SVM for Facial Expression Recognition</b>, <a href="https://github.com/amineHorseman/facial-expression-recognition-svm">https://github.com/amineHorseman/facial-expression-recognition-svm</a>, 2017.
    </li>
    <li>
        B. Farnsworth, <b>Facial Action Coding System (FACS), A Visual Guidebook</b> <a href ="https://imotions.com/blog/facial-action-coding-system/">https://imotions.com/blog/facial-action-coding-system/</a> , 2016.
    </li>
    <li>
        <b>Emotion Recognition in the Wild Challenge</b> <a href="http://icmi.acm.org/2017/index.php?id=challenges">http://icmi.acm.org/2017/index.php?id=challenges</a>, 2017.
    </li>
    <li>
        Joint Challenge on Dominant and Complementary Emotion Recognition Using Micro Emotion Features and Head-Pose Estimation,
        <a href="http://www.fg2017.org/index.php/challenges/">http://www.fg2017.org/index.php/challenges/</a>, 2017.
    </li>
    <li>
        Multimodal Emotion Recognition Challenge (MEC 2017), <a href="http://www.chineseldc.org/htdocsEn/emotion.html">
        http://www.chineseldc.org/htdocsEn/emotion.html</a>, 2017.
    </li>
    <li>
        Group-level Emotion Recognition (EmotiW 5.0) ICMI 2017,
        <a href="https://sites.google.com/site/emotiwchallenge/challenge-details">https://sites.google.com/site/emotiwchallenge/challenge-details</a>
    </li>

    <li>
        D. Amin, P. Chase, K. Sinha, <b>Touchy Feely: An Emotion Recognition Challenge</b>, Stanford CS231 Project Report, 2017.
    </li>

</ol>
</div>

<hr>
<h3>
    Appendix
</h3>
<p>
Division of work:
</p>
<p>
Davide: Action units extraction and classification using SVM for FER2013 and KDEF datasets, FER-2013 emotion classification using 3 layer CNNs.  
</p>
<p>
Mona: Acquiring and cleaning AffectNet Dataset, Setting up OpenFace Cambridge Docker on CentOS, Extracting AUs and landmarks from OpenFace for 500k AffectNet dataset,
setting up and running the baseline SVM on FER2013 dataset, taking care of the debate video, related work study.
</p>
<p>
Silvia: FER2013 emotion classification using 3 layer CNNs and VGG16, AffectNet using VGG16. 
</p>

<p>Below tables are from [14].</p>

<table style="width: 100%;" border="1">
    <tbody>
    <tr style="height: 21px;">
        <td style="height: 21px; width: 10%;"><strong>Action Unit</strong></td>
        <td style="height: 21px; width: 15%;"><strong>Description</strong></td>
        <td style="height: 21px; width: 31.3582%;"><strong>Facial Muscle</strong></td>
        <td style="height: 21px; width: 38.6418%;"><strong>Example</strong></td>
    </tr>
    <tr style="height: 21px;">
        <td style="height: 21px; width: 10%;"> 1</td>
        <td style="height: 21px; width: 15%;">Inner Brow Raiser</td>
        <td style="height: 21px; width: 31.3582%;"><em>Frontalis, pars medialis</em></td>
        <td style="height: 21px; width: 38.6418%;"><img class="aligncenter size-full wp-image-12727" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU1-FACS.gif" alt="AU1 FACS" width="300" height="150" /></td>
    </tr>
    <tr style="height: 21px;">
        <td style="height: 21px; width: 10%;"> 2</td>
        <td style="height: 21px; width: 15%;">Outer Brow Raiser (unilateral, right side)</td>
        <td style="height: 21px; width: 31.3582%;"><em>Frontalis, pars lateralis</em></td>
        <td style="height: 21px; width: 38.6418%;"><img class="aligncenter size-full wp-image-12728" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU2-right-only.gif" alt="AU2 right only FACS" width="300" height="150" /></td>
    </tr>
    <tr style="height: 21px;">
        <td style="height: 21px; width: 10%;"> 4</td>
        <td style="height: 21px; width: 15%;">Brow Lowerer</td>
        <td style="height: 21px; width: 31.3582%;"><em>Depressor Glabellae, Depressor Supercilli, Currugator</em></td>
        <td style="height: 21px; width: 38.6418%;"><img class="aligncenter size-medium wp-image-12380" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU4-brow-lowerer.gif" alt="au4 brow lowerer" width="300" height="150"  /></td>
    </tr>
    <tr style="height: 21px;">
        <td style="height: 21px; width: 10%;"> 5</td>
        <td style="height: 21px; width: 15%;">Upper Lid Raiser</td>
        <td style="height: 21px; width: 31.3582%;"><em>Levator palpebrae superioris</em></td>
        <td style="height: 21px; width: 38.6418%;"><img class="aligncenter size-full wp-image-12729" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU5.gif" alt="AU5 FACS" width="300" height="150" /></td>
    </tr>
    <tr style="height: 21px;">
        <td style="height: 21px; width: 10%;"> 6</td>
        <td style="height: 21px; width: 15%;">Cheek Raiser</td>
        <td style="height: 21px; width: 31.3582%;"><em>Orbicularis oculi, pars orbitalis</em></td>
        <td style="height: 21px; width: 38.6418%;"><img class="aligncenter size-medium wp-image-12383" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU6-cheek-raiser.gif" alt="AU6 cheek raiser" width="300" height="150"  /></td>
    </tr>
    <tr style="height: 21px;">
        <td style="height: 21px; width: 10%;"> 7</td>
        <td style="height: 21px; width: 15%;">Lid Tightener</td>
        <td style="height: 21px; width: 31.3582%;"><em>Orbicularis oculi, pars palpebralis</em></td>
        <td style="height: 21px; width: 38.6418%;"><img class="aligncenter size-medium wp-image-12384" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU7-lid-tightener.gif" alt="AU7 lid tightener" width="300" height="150" /></td>
    </tr>
    <tr style="height: 21px;">
        <td style="height: 21px; width: 10%;"> 9 (also shows slight AU4 and AU10)</td>
        <td style="height: 21px; width: 15%;">Nose Wrinkler</td>
        <td style="height: 21px; width: 31.3582%;"> <em>Levator labii superioris alaquae nasi</em></td>
        <td style="height: 21px; width: 38.6418%;"><img class="aligncenter size-full wp-image-12730" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU9-with-410.gif" alt="AU9 with 4+10" width="300" height="150" /></td>
    </tr>
    <tr style="height: 21px;">
        <td style="height: 21px; width: 10%;"> 10 (also shows slight AU25)</td>
        <td style="height: 21px; width: 15%;">Upper Lip Raiser</td>
        <td style="height: 21px; width: 31.3582%;"><em> Levator Labii Superioris, Caput infraorbitalis</em></td>
        <td style="height: 21px; width: 38.6418%;"><img class="aligncenter size-full wp-image-12731" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU10-with-25.gif" alt=" AU10 with 25" width="300" height="100" /></td>
    </tr>
    <tr style="height: 21px;">
        <td style="height: 21px; width: 10%;"> 12</td>
        <td style="height: 21px; width: 15%;">Lip Corner Puller</td>
        <td style="height: 21px; width: 31.3582%;"><em>Zygomatic Major</em></td>
        <td style="height: 21px; width: 38.6418%;"><img class="aligncenter size-full wp-image-12733" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU12.gif" alt="AU12" width="300" height="150" /></td>
    </tr>
    <tr style="height: 21px;">
        <td style="height: 21px; width: 10%;"> 14</td>
        <td style="height: 21px; width: 15%;">Dimpler</td>
        <td style="height: 21px; width: 31.3582%;"><em>Buccinator</em></td>
        <td style="height: 21px; width: 38.6418%;"><img class="aligncenter size-medium wp-image-12390" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU14-dimpler.gif" alt="AU14 dimpler" width="300" height="150"  /></td>
    </tr>
    <tr style="height: 21px;">
        <td style="height: 21px; width: 10%;"> 15</td>
        <td style="height: 21px; width: 15%;">Lip Corner Depressor</td>
        <td style="height: 21px; width: 31.3582%;"><em>Depressor anguli oris (Triangularis)</em></td>
        <td style="height: 21px; width: 38.6418%;"><img class="aligncenter size-full wp-image-12734" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU15.gif" alt="AU15 FACS" width="300" height="150" /></td>
    </tr>
    <tr style="height: 21px;">
        <td style="height: 21px; width: 10%;"> 17</td>
        <td style="height: 21px; width: 15%;">Chin Raiser</td>
        <td style="height: 21px; width: 31.3582%;"><em>Mentalis</em></td>
        <td style="height: 21px; width: 38.6418%;"><img class="aligncenter size-full wp-image-12736" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU17.gif" alt="AU17 FACS guide" width="300" height="150" /></td>
    </tr>
    <tr style="height: 21px;">
        <td style="height: 21px; width: 10%;"> 20</td>
        <td style="height: 21px; width: 15%;">Lip stretcher</td>
        <td style="height: 21px; width: 31.3582%;"><em>Risorius </em></td>
        <td style="height: 21px; width: 38.6418%;"><img class="aligncenter size-medium wp-image-12395" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU20-lip-stretcher.gif" alt="AU20 lip stretcher" width="300" height="150"  /></td>
    </tr>
    <tr style="height: 21px;">
        <td style="height: 21px; width: 10%;"> 23</td>
        <td style="height: 21px; width: 15%;">Lip Tightener</td>
        <td style="height: 21px; width: 31.3582%;"><em>Orbicularis oris</em></td>
        <td style="height: 21px; width: 38.6418%;"><img class="aligncenter size-medium wp-image-12397" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU23-lip-tightener.gif" alt="AU23 lip tightener" width="300" height="150"  /></td>
    </tr>
    <tr style="height: 21px;">
        <td style="height: 21px; width: 10%;"> 25</td>
        <td style="height: 21px; width: 15%;">Lips part</td>
        <td style="height: 21px; width: 31.3582%;"><em>Depressor Labii, Relaxation of Mentalis (AU17), Orbicularis Oris</em></td>
        <td style="height: 21px; width: 38.6418%;"><img class="aligncenter size-medium wp-image-12399" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU25-lips-part.gif" alt="AU25 lips part" width="300" height="150" /></td>
    </tr>
    <tr style="height: 21px;">
        <td style="height: 21px; width: 10%;"> 26</td>
        <td style="height: 21px; width: 15%;">Jaw Drop</td>
        <td style="height: 21px; width: 31.3582%;"><em>Masetter; Temporal and Internal Pterygoid </em>relaxed</td>
        <td style="height: 21px; width: 38.6418%;"><img class="aligncenter size-full wp-image-12740" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU26-with-25.gif" alt="AU26 with 25 FACS affectiva" width="300" height="150" /></td>
    </tr>
    <tr style="height: 21px;">
        <td style="height: 21px; width: 10%;"> 28</td>
        <td style="height: 21px; width: 15%;">Lip Suck</td>
        <td style="height: 21px; width: 31.3582%;"><em>Orbicularis oris</em></td>
        <td style="height: 21px; width: 38.6418%;"><img class="aligncenter size-full wp-image-12741" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU28-with-26.gif" alt="AU28 with 26 FACS affectiva" width="300" height="150" /></td>
    </tr>
    <tr style="height: 21px;">
        <td style="height: 21px; width: 10%;"> 45</td>
        <td style="height: 21px; width: 15%;">Blink</td>
        <td style="height: 21px; width: 31.3582%;">Relaxation of <em>Levator Palpebrae and Contraction of Orbicularis Oculi, Pars Palpebralis.</em></td>
        <td style="height: 21px; width: 38.6418%;"><img class="aligncenter size-medium wp-image-12407" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU45-blink.gif" alt=" AU45 blink" width="300" height="150"  /></td>
    </tr>
    </tbody>
</table>




<p>
    Each emotion can be associated to a set of AUs. Some examples are given in the table below.
</p>
<p>
    <br>
    <br>
<table style="width: 100%;" border="1">
    <tbody>
    <tr style="height: 21px;">
        <td style="height: 21px; width: 10%;"><b>Emotion</b></td>
        <td style="height: 21px; width: 15%;"><strong>Action Units</strong></td>
        <td style="height: 21px; width: 31.3582%;"><strong>Description</strong></td>
        <td style="height: 21px; width: 38.6418%;"><strong>Examples (Hover to Play)</strong></td>
    </tr>
    <tr style="height: 21px;">
        <td style="height: 21px; width: 10%;">Happiness / Joy</td>
        <td style="height: 21px; width: 15%;">6 + 12</td>
        <td style="height: 21px; width: 31.3582%;">Cheek Raiser, Lip Corner Puller</td>
        <td style="height: 21px; width: 38.6418%;"><img class="size-medium wp-image-12383 aligncenter" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU6-cheek-raiser.gif" alt="AU6 cheek raiser" width="300" height="150" /></p>
<p><img class="size-medium wp-image-12388 aligncenter" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU12-lip-corner-puller.gif" alt="AU12 lip corner puller" width="300" height="150"  /></td>
    </tr>
    <tr style="height: 21px;">
        <td style="height: 21px; width: 10%;">Sadness</td>
        <td style="height: 21px; width: 15%;">1 + 4 + 15</td>
        <td style="height: 21px; width: 31.3582%;">Inner Brow Raiser, Brow Lowerer, Lip Corner Depressor</td>
        <td style="height: 21px; width: 38.6418%;"><img class="aligncenter size-medium wp-image-12378" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU1-inner-brow-raiser.gif" alt="AU1 inner brow raiser" width="300" height="150" /></p>
<p><img class="aligncenter size-medium wp-image-12380" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU4-brow-lowerer.gif" alt="au4 brow lowerer" width="300" height="150" /></p>
<p><img class="aligncenter size-medium wp-image-12391" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU15-lip-corner-depressor.gif" alt="AU15 lip corner depressor" width="300" height="150" /></td>
    </tr>
    <tr style="height: 21px;">
        <td style="height: 21px; width: 10%;">Surprise</td>
        <td style="height: 21px; width: 15%;">1 + 2 + 5 + 26</td>
        <td style="height: 21px; width: 31.3582%;">Inner Brow Raiser, Outer Brow Raiser, Upper Lid Raiser, Jaw Drop</td>
        <td style="height: 21px; width: 38.6418%;"><img class="aligncenter size-medium wp-image-12378" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU1-inner-brow-raiser.gif" alt="AU1 inner brow raiser" width="300" height="150"  /></p>
<p><img class="aligncenter size-medium wp-image-12379" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU2-outer-brow-raiser.gif" alt="au2 outer brow raiser" width="300" height="150" /></p>
<p><img class="aligncenter size-medium wp-image-12382" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU5-upper-lid-raiser.gif" alt="au5 upper lid raiser" width="300" height="150" /></p>
<p><img class="aligncenter size-medium wp-image-12400" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU26-jaw-drop.gif" alt="AU26 jaw drop" width="300" height="150" /></td>
    </tr>
    <tr style="height: 21px;">
        <td style="height: 21px; width: 10%;">Fear</td>
        <td style="height: 21px; width: 15%;">1 + 2 + 4 + 5 + 7 + 20 + 26</td>
        <td style="height: 21px; width: 31.3582%;">Inner Brow Raiser, Outer Brow Raiser, Brow Lowerer, Upper Lid Raiser, Lid Tightener, Lip Stretcher, Jaw Drop</td>
        <td style="height: 21px; width: 38.6418%;"><img class="aligncenter size-medium wp-image-12378" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU1-inner-brow-raiser.gif" alt="AU1 inner brow raiser" width="300" height="150" /></p>
<p><img class="aligncenter size-medium wp-image-12379" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU2-outer-brow-raiser.gif" alt="au2 outer brow raiser" width="300" height="150"  /></p>
<p><img class="aligncenter size-medium wp-image-12380" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU4-brow-lowerer.gif" alt="au4 brow lowerer" width="300" height="150" /></p>
<p><img class="aligncenter size-medium wp-image-12382" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU5-upper-lid-raiser.gif" alt="au5 upper lid raiser" width="300" height="150" /></p>
<p><img class="aligncenter size-medium wp-image-12384" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU7-lid-tightener.gif" alt="AU7 lid tightener" width="300" height="150"  /></p>
<p><img class="aligncenter size-medium wp-image-12395" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU20-lip-stretcher.gif" alt="AU20 lip stretcher" width="300" height="150" /></p>
<p><img class="aligncenter size-medium wp-image-12400" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU26-jaw-drop.gif" alt="AU26 jaw drop" width="300" height="150" /></td>
    </tr>
    <tr style="height: 21px;">
        <td style="height: 21px; width: 10%;">Anger</td>
        <td style="height: 21px; width: 15%;">4 + 5 + 7 + 23</td>
        <td style="height: 21px; width: 31.3582%;">Brow Lowerer, Upper Lid Raiser, Lid Tightener, Lip Tightener</td>
        <td style="height: 21px; width: 38.6418%;"><img class="aligncenter size-medium wp-image-12380" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU4-brow-lowerer.gif" alt="au4 brow lowerer" width="300" height="150"  /></p>
<p><img class="aligncenter size-medium wp-image-12382" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU5-upper-lid-raiser.gif" alt="au5 upper lid raiser" width="300" height="150"/></p>
<p><img class="aligncenter size-medium wp-image-12384" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU7-lid-tightener.gif" alt="AU7 lid tightener" width="300" height="150"/></p>
<p><img class="aligncenter size-medium wp-image-12397" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU23-lip-tightener.gif" alt="AU23 lip tightener" width="300" height="150" /></td>
    </tr>
    <tr style="height: 21px;">
        <td style="height: 21px; width: 10%;">Disgust</td>
        <td style="height: 21px; width: 15%;">9 + 15 + 16</td>
        <td style="height: 21px; width: 31.3582%;">Nose Wrinkler, Lip Corner Depressor, Lower Lip Depressor</td>
        <td style="height: 21px; width: 38.6418%;"><img class="aligncenter size-medium wp-image-12385" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU9-nose-wrinkler.gif" alt="AU9 nose wrinkler" width="300" height="150"  /></p>
<p><img class="aligncenter size-medium wp-image-12391" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU15-lip-corner-depressor.gif" alt="AU15 lip corner depressor" width="300" height="150"  /></p>
<p><img class="aligncenter size-medium wp-image-12392" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU16-lower-lip-depressor.gif" alt="AU16 lower lip depressor" width="300" height="150"  /></td>
    </tr>
    <tr style="height: 21px;">
        <td style="height: 21px; width: 10%;">Contempt</td>
        <td style="height: 21px; width: 15%;">12 + 14 (on one side of the face)</td>
        <td style="height: 21px; width: 31.3582%;">Lip Corner Puller, Dimpler</td>
        <td style="height: 21px; width: 38.6418%;"><img class="aligncenter size-medium wp-image-12388" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU12-lip-corner-puller.gif" alt="AU12 lip corner puller" width="300" height="150" /></p>
<p><img class="aligncenter size-medium wp-image-12390" src="https://cdn.imotions.com/wp-content/uploads/2016/12/AU14-dimpler.gif" alt="AU14 dimpler" width="300" height="150" /></td>
    </tr>
    </tbody>
    </table>



</body>

</html>


